
<!DOCTYPE html>
<html lang="en">
<head>
  <title>Iterative Design</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
  <link rel="stylesheet" href="signo-fraud/styles.css" /> 
  <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Lato" rel="stylesheet" type="text/css">
</head>
<body id="body" data-spy="scroll" data-target=".navbar" data-offset="60">

  <nav class="navbar navbar-dark sticky-top py-3 navbar-expand-md" style="background-color: #020A12;">
    <div class="container-fluid navbar-container">
      <a class="navbar-brand" href="index.html">unrulypenguin642</a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation" onclick="toggleMenuIcon">
        <span class="navbar-toggler-icon"></span>
      </button>
      <div class="collapse navbar-collapse" id="navbar">
        <ul class="navbar-nav ms-auto mb-2">
          <li><a class="nav-link" href="#background">BACKGROUND</a></li>
          <li><a class="nav-link" href="#methods">METHODOLOGY</a></li>
          <li><a class="nav-link" href="#results">RESULTS</a></li>
          <li><a class="nav-link" href="#discussion">DISCUSSION</a></li>
          <li><a class="nav-link" href="#summary">SUMMARY</a></li>
        </ul>
      </div>  
    </div>
  </nav>


<div id="title" class="container-fluid text-center">
  <br>
  <br>
  <h1>Sig-No Fraud: Signature forgery detection using Siamese Convolutional Neural Networks </h1>
</div>

<!-- BACKGROUND -->
<div id="background" class="container-fluid">
  <div class="row">
    <div class="col-sm-12">
      <h2>BACKGROUND</h2>
      <h4>
        Signature verification is historically a challenging task in biometrics and document forensics due to the possibility of small details that a fraudulent signature may contain. We used an existing paper to implement a Siamese convolutional neural network, coined SigNet, a twin network with shared weights that observes similar observations placed in proximity in order to learn a feature space. By exposing the network to similar and dissimilar observations, the model tries to minimize the Euclidean distance between similar pairs while simultaneously maximizing it between dissimilar pairs. The paper was capable of detecting forgery in different languages and handwriting styles, classifying signatures as genuine or forged. 
      </h4>
    </div>
  </div>
</div>

<!-- METHODOLOGY -->
<div id="methods" class="container-fluid bg-grey">
  <h2 class="center-header">Methodology</h2>

  <br>
  <div class="row">
    <div class="col-sm-12">
      <h4><strong> Preprocessing </strong></h4>
      <h4>
            We used 3 datasets:
      </h4>
      <ol>
        <li>CEDAR: 1320 forged & 1320 genuine English signatures</li>
        <li>BHSig260: 7800 forged & 6240 Bengali or Hindu signatures</li>
        <li>mchen77: 24 forged & 24 genuine English signatures</li>
      </ol>
        <h4>
            In order to convert the signatures to a usable format, we converted each image into a Numpy array with the shape (155, 220), where each index has a value from 0-255. Because the network takes in two images as the input, we also created pairs of genuine/genuine and genuine/forged signatures. 
        </h4>
    </div>
  </div>
  <br>
  <div class="row">
    <div class="col-sm-12">
      <h4><strong> Architecture </strong></h4>
      <img class="img-fluid" src="signo-fraud/architecture.PNG" alt="Model Architecture">
        <h4>
            We followed the same model architecture as the Signet research paper, which can be seen in the image above. This is a Siamese convolutional neural network, which is a network that contains two identical CNN subnetworks. The parameters in the subnetwork have mirrored updates. A contrastive loss function is used, so that it will calculate the similarity using Euclidean distance. Given two signatures, the model will output that it is genuine if the signatures are similar, and output false if the signatures are dissimilar. 
        </h4>
    </div>
  </div>
  <br>
  <div class="row">
    <div class="col-sm-12">
      <h4><strong> Hyperparameters </strong></h4>
      <h4>
        We used two sets of hyperparameters for our model: 
      </h4>
    </div>
    <div class="col-sm-6">
        <h4>RMSProp</h4>
        <ul>
            <li>batch_size: 32</li>
            <li>activation: relu</li>
            <li>learning_rate: 0.0001</li>
            <li>rho: 0.9</li>
            <li>epsilon: 1e-8</li>
            <li>optimizer: RMSProp</li>
            <li>epochs: 100</li>
        </ul>
    </div>
    <div class="col-sm-6">
        <h4>Adam</h4>
        <ul>
            <li>batch_size: 32</li>
            <li>activation: relu</li>
            <li>learning_rate: 0.001</li>
            <li>beta_1: 0.9</li>
            <li>beta_2: 0.999</li>
            <li>epsilon: 1e-8</li>
            <li>optimizer: Adam</li>
            <li>epochs: 100</li>
        </ul>
    </div>
  </div>
  <br>
  <div class="row">
    <div class="col-sm-12">
        <h4><strong> Training & Testing </strong></h4>
        <h4>
            We trained the model by passing in pairs of genuine/genuine signatures and genuine/forged signatures into the network, and let Tensorflow update the weights for 100 epochs. For gradient descent, we used the following contrastive loss function, since it will bring images of the same class closer together: 
        </h4>
        <div class="text-center">
            <img class="img-fluid" src="signo-fraud/loss.PNG" alt="Model Architecture">
        </div>
        <h4>
            During testing, we found the threshold and maximum accuracy of the trained model using the testing set. The threshold is a value representing the maximum distance two signatures can have and still be similar. The threshold was found by finding the value that maximized the accuracy of the model. 
        </h4>
    </div>
  </div>
</div>

<!-- RESULTS -->
<div id="results" class="container-fluid text-center">
  <h2>Results</h2>

  <h4>We first trained and tested the model using the same optimizer as the research paper, RMSprop. Below are the results: </h4>
  <img class="img-fluid" src="signo-fraud/rmsprop-results.PNG">

  <h4>The accuracy of our model was lower than that of the research paper (100%). However, we were able to achieve much greater accuracy by using a different optimizer, Adam. Below are the results: </h4>
  <img class="img-fluid" src="signo-fraud/adam-results.PNG">
  <h4>We also tested the model on a group member’s signature, and it was able to detect that the signature was forged.</h4>
  <div class="row">
    <div class="col-sm-6">
        <img class="img-fluid" src="signo-fraud/forged.PNG">
      </div>
      <div class="col-sm-6">
        <img class="img-fluid" src="signo-fraud/genuine.PNG">
      </div>
  </div>
</div>

<!-- DISCUSSION -->
<div id="discussion" class="container-fluid bg-grey">
  <h2>Discussion</h2>
  <h4>We were able to achieve an accuracy of 0.99 using the Adam optimizer, and an accuracy of 0.86 using the RMSprop optimizer, which is slightly lower than the accuracy achieved by the research paper. This could be attributed to using more datasets, since we additionally used the BHSig260 dataset, which has non-English signatures (Hindu and Bengali).</h4>
  <h4>The model trained by the Adam optimizer had a higher accuracy and a lower rate of false negatives, but a higher rate of false positives, compared to that of the model trained using the RMSprop optimizer. This was likely due to the lower difference threshold it found. </h4>
  <div class="row">
    <div class="col-sm-12">
      <h4><b>Challenges</b></h4>
      <h4>Throughout this project, we ran into various challenges:</h4>
      <ul>
        <li><b>Preprocessing:</b> Preprocessing required a lot of careful manipulation of types and shapes to get the ideal preprocessed data to be run. These parameters, such as image dimensions and train/test/validation batching can be altered to yield better results, but we also needed to be aware of our own compute limitations when tuning these parameters.</li>
        <li><b>Model Implementation:</b> Although the research paper had a good diagram for showing the different parts of the model, it was difficult to apply it to actual code. Additionally, linking up the preprocessed data so that it works on the model’s functions was challenging, since we didn’t have to work on that part in previous assignments. </li>
        <li><b>Large Datasets: </b>Because the datasets that we used were large, it took a long time to train and test the model. We were unable to use the entire BHSig260 dataset, because we were limited by the amount of computing power. </li>
      </ul>
    </div>
  </div>
</div>

<!-- SUMMARY -->
<div id="summary" class="container-fluid">
  <div class="row">
    <div class="col-sm-12">
      <h2>Summary & Takeaways</h2>
      <h4> In this project, we implemented a Siamese convolutional neural network to detect forged signatures, which obtained over 90% accuracy. We had the following takeaways:</h4>
      <ul>
        <li> We learned a lot more about Siamese convolutional neural networks and contrastive loss functions, which we had never used before.</li>
        <li>Preprocessing data is a very difficult and lengthy task, especially when dealing with image data because of the amount of reshaping that needs to be done.</li>
        <li> Training and testing neural network models take lots of time! If we had more time, we could have tried more optimizations and tested our model on a wider range of datasets. Different loss functions could also be explored other than Euclidean distance, such as cosine similarity. 
        </li>
        <li>Furthermore, our current model focuses on offline signature forgery detection, but we could also extend this to electronic, or “online”, signatures which also take into account stroke order and speed. </li>
      </ul>
    </div>
  </div>
</div>

</body>
</html>
